{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7741261,"sourceType":"datasetVersion","datasetId":4524688}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers==4.18.0\n!pip install numpy==1.19.5\n!pip install torch==1.7.1\n!pip install scikit-learn==0.20.3","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nfrom transformers import BertTokenizer, Trainer, BertForSequenceClassification, TrainingArguments\nfrom datasets import Dataset\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/gooddata/Preprocessed.csv')\ndf = df.dropna()\nlabel_encoder = LabelEncoder()\ndf['label'] = label_encoder.fit_transform(df['label'])\ndf","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split dataset into training, validation, and testing sets\ndf_train, df_test = train_test_split(df, test_size=0.1, random_state=42)\ndf_train, df_val = train_test_split(df_train, test_size=0.1, random_state=42)\n\n# Load the FinBERT pretrained model\nmodel = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-pretrain', num_labels=3) # Assuming you have 3 labels: positive, negative, neutral\ntokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-pretrain')\n\n# Prepare datasets for fine-tuning\ndataset_train = Dataset.from_pandas(df_train)\ndataset_val = Dataset.from_pandas(df_val)\ndataset_test = Dataset.from_pandas(df_test)\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenize datasets\ndef tokenize_data(example):\n    return tokenizer(example['text'], truncation=True, padding='max_length', max_length=128)\n\ndataset_train = dataset_train.map(tokenize_data, batched=True)\ndataset_val = dataset_val.map(tokenize_data, batched=True)\ndataset_test = dataset_test.map(tokenize_data, batched=True)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set dataset format\ndataset_train.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\ndataset_val.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\ndataset_test.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define training options and metrics\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    return {'accuracy' : accuracy_score(predictions, labels)}\n\nargs = TrainingArguments(\n    output_dir='temp/',\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    learning_rate=2e-5,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    metric_for_best_model='accuracy',\n)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=dataset_train,\n    eval_dataset=dataset_val,\n    compute_metrics=compute_metrics\n)\n\n# Train the model\ntrainer.train()","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]}]}